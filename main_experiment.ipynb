{
 "cells": [
  {
   "cell_type": "code",
   "id": "735019c6165dae6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T03:07:09.147269Z",
     "start_time": "2025-05-01T03:07:02.091174Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def load_triviaqa(dataset_path: str) -> Tuple[Dict[str, str], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Load TriviaQA dataset and return documents and QA pairs.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the TriviaQA JSON file (e.g., 'unfiltered-dev.json')\n",
    "        max_docs: Maximum number of documents to load\n",
    "        max_qa_pairs: Maximum number of QA pairs to load\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (documents dictionary, list of QA pairs)\n",
    "    \"\"\"\n",
    "    print(f\"Loading TriviaQA dataset from: {dataset_path}\")\n",
    "    \n",
    "    # Load the TriviaQA dataset\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)['Data']\n",
    "    \n",
    "    # Extract documents and QA pairs\n",
    "    docs = {}\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        if len(docs)>100:\n",
    "            break\n",
    "        question = item['Question']\n",
    "        answers = item['Answer']['NormalizedAliases']\n",
    "        \n",
    "        # Use the first answer as the gold answer\n",
    "        gold_answer = answers[0] if answers else item['Answer']['NormalizedValue']\n",
    "        \n",
    "        qa_pairs.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": gold_answer\n",
    "        })\n",
    "        \n",
    "        # Extract documents from Wikipedia sources\n",
    "        if 'EntityPages' in item:\n",
    "            for j, doc_info in enumerate(item['EntityPages']):\n",
    "                    \n",
    "                doc_id = f\"wiki_{doc_info['Title']}_{j}\"\n",
    "                \n",
    "                # Check if file exists\n",
    "                if 'Filename' in doc_info:\n",
    "                    doc_path = os.path.join(os.path.dirname(\"data/wikipedia\"), doc_info['Filename'])\n",
    "                    if os.path.exists(doc_path):\n",
    "                        try:\n",
    "                            with open(doc_path, 'r', encoding='utf-8') as doc_file:\n",
    "                                doc_content = doc_file.read()\n",
    "                                docs[doc_id] = doc_content\n",
    "\n",
    "                        except:\n",
    "                            # If file can't be read, use the snippet\n",
    "                            docs[doc_id] = doc_info.get('Snippet', '')\n",
    "                    else:\n",
    "                        # If file doesn't exist, use the snippet\n",
    "                        docs[doc_id] = doc_info.get('Snippet', '')\n",
    "                else:\n",
    "                    # If no filename, use the snippet\n",
    "                    docs[doc_id] = doc_info.get('Snippet', '')\n",
    "                    \n",
    "        # Add web documents if available\n",
    "        if 'SearchResults' in item:\n",
    "            for j, doc_info in enumerate(item['SearchResults']):\n",
    "                    \n",
    "                doc_id = f\"web_{j}_{doc_info.get('Title', '')}\"\n",
    "                \n",
    "                if 'Filename' in doc_info:\n",
    "                    doc_path = os.path.join(\"data/web\", doc_info['Filename'])\n",
    "                    if os.path.exists(doc_path):\n",
    "                        try:\n",
    "                            with open(doc_path, 'r', encoding='utf-8') as doc_file:\n",
    "                                doc_content = doc_file.read()\n",
    "                                docs[doc_id] = doc_content\n",
    "                        except:\n",
    "                            docs[doc_id] = doc_info.get('Snippet', '')\n",
    "                    else:\n",
    "                        docs[doc_id] = doc_info.get('Snippet', '')\n",
    "                else:\n",
    "                    docs[doc_id] = doc_info.get('Snippet', '')\n",
    "    \n",
    "    print(f\"Loaded {len(docs)} documents and {len(qa_pairs)} QA pairs from TriviaQA\")\n",
    "    return docs, qa_pairs\n",
    "\n",
    "documents_dict, qa_pairs = load_triviaqa(\"triviaqa-unfiltered/unfiltered-web-dev.json\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TriviaQA dataset from: triviaqa-unfiltered/unfiltered-web-dev.json\n",
      "Loaded 101 documents and 2 QA pairs from TriviaQA\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T03:07:10.823755Z",
     "start_time": "2025-05-01T03:07:10.816526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "def benchmark_retrieval(retriever, query, top_k):\n",
    "    start = time.time()\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    end = time.time()\n",
    "    latency = end - start\n",
    "    return results, latency\n",
    "\n",
    "def benchmark_all(strategies, documents, query, top_k=5):\n",
    "    results = []\n",
    "\n",
    "    for chunker_name, chunker in strategies[\"chunkers\"].items():\n",
    "        chunks = []\n",
    "        for doc in documents:\n",
    "            chunks.extend(chunker.chunk(doc))\n",
    "\n",
    "        for retriever_name, retriever_class in strategies[\"retrievers\"].items():\n",
    "            retriever = retriever_class(chunks)\n",
    "            retrieved_docs, latency = benchmark_retrieval(retriever, query, top_k)\n",
    "\n",
    "            results.append({\n",
    "                \"chunker\": chunker_name,\n",
    "                \"retriever\": retriever_name,\n",
    "                \"latency\": latency,\n",
    "                \"results\": retrieved_docs\n",
    "            })\n",
    "\n",
    "    return results\n"
   ],
   "id": "abedc14f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "a00d9182",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-05-01T03:10:34.547710Z",
     "start_time": "2025-05-01T03:07:22.708734Z"
    }
   },
   "source": [
    "from chunkers import FixedChunker, OverlappingChunker, SemanticChunker\n",
    "from retrievers.bm25_retriever import BM25Retriever\n",
    "from retrievers.dense_retriever import DPRRetriever\n",
    "from retrievers.colbert_retriever import ColBERTRetriever\n",
    "from retrievers.hybrid_retriever import HybridRetriever\n",
    "\n",
    "documents = list(documents_dict.values())\n",
    "query = \"What are Critics seemed focused on?\"\n",
    "top_k = 5\n",
    "\n",
    "strategies = {\n",
    "    \"chunkers\": {\n",
    "        \"fixed\": FixedChunker(chunk_size=20, drop_last=False),\n",
    "        \"overlapping\": OverlappingChunker(chunk_size=24, overlap=8, drop_last=False),\n",
    "        \"semantic\": SemanticChunker(chunk_char_limit=120)\n",
    "    },\n",
    "    \"retrievers\": {\n",
    "        # \"bm25\": BM25Retriever,\n",
    "        # \"dpr\": DPRRetriever,\n",
    "        \"colbert\": ColBERTRetriever,\n",
    "        # \"hybrid\": HybridRetriever\n",
    "    }\n",
    "}\n",
    "results = []\n",
    "for qa in qa_pairs:\n",
    "    query, gold = qa[\"question\"], qa[\"answer\"]\n",
    "    result = benchmark_all(strategies, documents, query, top_k)\n",
    "    results.extend(result)\n",
    "\n",
    "# Print results\n",
    "for res in results:\n",
    "    print(f\"Chunker: {res['chunker']}, Retriever: {res['retriever']}, Latency: {res['latency']:.3f}s\")\n",
    "    print(\"Top Docs:\", res[\"results\"][:2])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a583a68580938af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T03:03:11.588152Z",
     "start_time": "2025-05-01T03:03:11.563165Z"
    }
   },
   "source": "print(qa_pairs)\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'Who was the man behind The Chipmunks?', 'answer': 'david seville'}]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd4fbc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed-length chunks (20 tokens each):\n",
      "  01. 'The sky appears blue because molecules in the air scatter blue light from the sun more than they scatter red'\n",
      "  02. 'light. This phenomenon is called Rayleigh scattering. When the sun is lower in the sky, the light has to pass'\n",
      "  03. 'through more atmosphere, so more blue and green light is scattered away, leaving the reds and oranges we see at'\n",
      "\n",
      "Overlapping chunks (24 tokens, 8-token overlap):\n",
      "  01. 'The sky appears blue because molecules in the air scatter blue light from the sun more than they scatter red light. This phenomenon is'\n",
      "  02. 'than they scatter red light. This phenomenon is called Rayleigh scattering. When the sun is lower in the sky, the light has to pass'\n",
      "  03. 'in the sky, the light has to pass through more atmosphere, so more blue and green light is scattered away, leaving the reds and'\n",
      "  04. 'light is scattered away, leaving the reds and oranges we see at sunrise and sunset.'\n",
      "\n",
      "Semantic chunks (~120 chars, sentence-aware):\n",
      "  01. 'The sky appears blue because molecules in the air scatter blue light from the sun more than they scatter red light.'\n",
      "  02. 'This phenomenon is called Rayleigh scattering.'\n",
      "  03. 'When the sun is lower in the sky, the light has to pass through more atmosphere, so more blue and green light is scattered away, leaving the reds and oranges we see at sunrise and sunset.'\n"
     ]
    }
   ],
   "source": [
    "# --- Example text ----------------------------------------------------------\n",
    "doc = (\n",
    "    \"The sky appears blue because molecules in the air scatter blue light from \"\n",
    "    \"the sun more than they scatter red light. This phenomenon is called Rayleigh \"\n",
    "    \"scattering. When the sun is lower in the sky, the light has to pass through \"\n",
    "    \"more atmosphere, so more blue and green light is scattered away, leaving the \"\n",
    "    \"reds and oranges we see at sunrise and sunset.\"\n",
    ")\n",
    "\n",
    "# --- Import the chunkers ---------------------------------------------------\n",
    "from chunkers import FixedChunker, OverlappingChunker, SemanticChunker\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Fixed-length chunking: split every 20 tokens (drop tail if < 20 tokens)\n",
    "fixed_chunker = FixedChunker(chunk_size=20, drop_last=True)\n",
    "fixed_chunks = fixed_chunker.chunk(doc)\n",
    "\n",
    "# 2. Overlapping windows: 24-token windows with 8-token overlap\n",
    "overlap_chunker = OverlappingChunker(chunk_size=24, overlap=8, drop_last=False)\n",
    "overlap_chunks = overlap_chunker.chunk(doc)\n",
    "\n",
    "# 3. Semantic packing: keep whole sentences, ~120 characters per chunk\n",
    "semantic_chunker = SemanticChunker(chunk_char_limit=120)\n",
    "semantic_chunks = semantic_chunker.chunk(doc)\n",
    "\n",
    "# --- Inspect the output ----------------------------------------------------\n",
    "print(\"Fixed-length chunks (20 tokens each):\")\n",
    "for i, c in enumerate(fixed_chunks, 1):\n",
    "    print(f\"  {i:02d}. {c!r}\")\n",
    "\n",
    "print(\"\\nOverlapping chunks (24 tokens, 8-token overlap):\")\n",
    "for i, c in enumerate(overlap_chunks, 1):\n",
    "    print(f\"  {i:02d}. {c!r}\")\n",
    "\n",
    "print(\"\\nSemantic chunks (~120 chars, sentence-aware):\")\n",
    "for i, c in enumerate(semantic_chunks, 1):\n",
    "    print(f\"  {i:02d}. {c!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39985b2e30f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 0.  Imports & one-time setup\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict\n",
    "import json, time\n",
    "\n",
    "# local modules\n",
    "from chunkers import FixedChunker, OverlappingChunker, SemanticChunker\n",
    "from retrievers import bm25_retriever, dense_retriever, hybrid_retriever\n",
    "from retrievers.dense_retriever import DPRRetriever  # example alias\n",
    "from utils.evaluation import exact_match, f1_score            # already in your repo\n",
    "from utils.timing import Timer                                 # already in your repo\n",
    "\n",
    "# external (install if missing)\n",
    "#   pip install rank-bm25 sentence-transformers faiss-cpu\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  Load a toy corpus\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "DOCS: Dict[str, str] = {\n",
    "    \"doc1\": Path(\"data/doc1.txt\").read_text(encoding=\"utf-8\"),\n",
    "    \"doc2\": Path(\"data/doc2.txt\").read_text(encoding=\"utf-8\"),\n",
    "    \"doc3\": Path(\"data/doc3.txt\").read_text(encoding=\"utf-8\"),\n",
    "}\n",
    "print(f\"Loaded {len(DOCS)} raw documents\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  Choose **one** chunker recipe for this run\n",
    "#     (Swap these objects in a loop if you’re doing a grid-search)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "chunker = FixedChunker(chunk_size=128, drop_last=False)\n",
    "# chunker = OverlappingChunker(chunk_size=256, overlap=64)\n",
    "# chunker = SemanticChunker(chunk_char_limit=1500)\n",
    "\n",
    "# 2-b.  Chunk every document → corpus_chunks[id] = [chunk0, …]\n",
    "corpus_chunks: Dict[str, List[str]] = {\n",
    "    doc_id: chunker.chunk(txt) for doc_id, txt in DOCS.items()\n",
    "}\n",
    "flat_chunks = [c for chs in corpus_chunks.values() for c in chs]\n",
    "print(\n",
    "    f\"Chunked into {len(flat_chunks):,} total passages \"\n",
    "    f\"(avg {len(flat_chunks)/len(DOCS):.1f} per doc)\"\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  Build the four retrieval back-ends on **exactly the same chunk set**\n",
    "#     The dense & hybrid examples assume a SentenceTransformer-based DPR.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3-a. BM25 (sparse)\n",
    "bm25_index = BM25Okapi([c.split() for c in flat_chunks])\n",
    "\n",
    "# 3-b. Dense Passage Retriever\n",
    "dpr_model = SentenceTransformer(\"facebook-dpr-ctx_encoder-multiset-base\")\n",
    "dpr_index = dense_retriever.build_faiss_index(flat_chunks, dpr_model)  # your util\n",
    "\n",
    "# 3-c. ColBERT (late interaction)  ← placeholder call\n",
    "colbert_retriever = dense_retriever.ColBERTIndexer().fit(flat_chunks)\n",
    "\n",
    "# 3-d. Hybrid (= BM25 + DPR scores)\n",
    "hybrid = hybrid_retriever.Hybrid(bm25_index, dpr_index, alpha=0.4)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4.  Query loop + simple QA evaluation\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "qa_pairs = json.loads(Path(\"data/dev_qas.json\").read_text())[:50]  # small dev slice\n",
    "K = 5\n",
    "\n",
    "results_by_system = defaultdict(list)\n",
    "\n",
    "for q in qa_pairs:\n",
    "    query, gold = q[\"question\"], q[\"answer\"]  # gold answer string\n",
    "\n",
    "    with Timer() as t:\n",
    "        top_chunks = bm25_index.get_top_n(query.split(), flat_chunks, n=K)\n",
    "    results_by_system[\"bm25\"].append(\n",
    "        {\"latency_ms\": t.ms, \"em\": exact_match(gold, top_chunks)}\n",
    "    )\n",
    "\n",
    "    with Timer() as t:\n",
    "        top_chunks = dense_retriever.search_faiss(query, dpr_index, dpr_model, top_k=K)\n",
    "    results_by_system[\"dpr\"].append(\n",
    "        {\"latency_ms\": t.ms, \"em\": exact_match(gold, top_chunks)}\n",
    "    )\n",
    "\n",
    "    with Timer() as t:\n",
    "        top_chunks = colbert_retriever.search(query, top_k=K)\n",
    "    results_by_system[\"colbert\"].append(\n",
    "        {\"latency_ms\": t.ms, \"em\": exact_match(gold, top_chunks)}\n",
    "    )\n",
    "\n",
    "    with Timer() as t:\n",
    "        top_chunks = hybrid.search(query, top_k=K)\n",
    "    results_by_system[\"hybrid\"].append(\n",
    "        {\"latency_ms\": t.ms, \"em\": exact_match(gold, top_chunks)}\n",
    "    )\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5.  Aggregate & display – how *this* chunker impacted each retriever\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def summarise(rows):\n",
    "    return {\n",
    "        \"EM\": sum(r[\"em\"] for r in rows) / len(rows),\n",
    "        \"Latency (ms)\": sum(r[\"latency_ms\"] for r in rows) / len(rows),\n",
    "    }\n",
    "\n",
    "summary = {name: summarise(rows) for name, rows in results_by_system.items()}\n",
    "print(\"\\n===  Chunker:\", chunker.__class__.__name__, \" ===\")\n",
    "for system, metrics in summary.items():\n",
    "    print(f\"{system:7s} | EM={metrics['EM']:.3f} | Latency≈{metrics['Latency (ms)']:.1f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4bb226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load] TriviaQA from data/triviaqa-unfiltered/unfiltered-web-dev.json\n",
      "[load] kept 250 non-empty docs, 100 QA pairs\n",
      "[chunk] FixedChunker: 250 chunks (≈1.0 per doc)\n",
      "[dpr] encoding chunks …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:01<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dpr] Faiss index: 250 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "retrieving: 100%|██████████| 100/100 [00:04<00:00, 22.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESULTS (Exact-Match evidence recall) ===\n",
      "BM25 | EM@5: 0.010 | avg latency 0.8 ms\n",
      "DPR  | EM@5: 0.000 | avg latency 43.2 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Minimal Retrieval-only demo on TriviaQA-unfiltered.\n",
    "\n",
    "Dependencies:\n",
    "  pip install rank-bm25 sentence-transformers faiss-cpu tqdm\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, gzip, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Retrieval libs\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss                           # CPU version is OK\n",
    "\n",
    "# Your chunkers (put the three modules in rag_pipeline/chunkers/)\n",
    "from chunkers import FixedChunker, OverlappingChunker, SemanticChunker\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 1. TriviaQA loader (improved)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def load_triviaqa(\n",
    "    dataset_path: str,\n",
    "    max_docs: int = 300,\n",
    "    max_qa_pairs: int = 100,\n",
    ") -> Tuple[Dict[str, str], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        docs   : {doc_id: raw_text}\n",
    "        qa_pairs: [{\"question\": str, \"answer\": str}, …]\n",
    "    \"\"\"\n",
    "    print(f\"[load] TriviaQA from {dataset_path}\")\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)[\"Data\"]\n",
    "\n",
    "    root = Path(dataset_path).parent\n",
    "    docs: Dict[str, str] = {}\n",
    "    qa_pairs: List[Dict] = []\n",
    "\n",
    "    def read_evidence(file_path: Path) -> str:\n",
    "        if not file_path.exists():\n",
    "            return \"\"\n",
    "        # Many evidence files are .gz\n",
    "        try:\n",
    "            if file_path.suffix == \".gz\":\n",
    "                with gzip.open(file_path, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as g:\n",
    "                    return g.read()\n",
    "            return file_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def add_doc(doc_info, prefix: str, j: int):\n",
    "        if len(docs) >= max_docs:\n",
    "            return\n",
    "        doc_id = f\"{prefix}_{j}\"\n",
    "        txt = \"\"\n",
    "        if doc_info.get(\"Filename\"):\n",
    "            txt = read_evidence(root / doc_info[\"Filename\"])\n",
    "        # Fallbacks\n",
    "        txt = txt or doc_info.get(\"Snippet\", \"\") or doc_info.get(\"Title\", \"\") or doc_info.get(\"Url\", \"\")\n",
    "        docs[doc_id] = txt\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        if i >= max_qa_pairs:\n",
    "            break\n",
    "\n",
    "        # QA\n",
    "        question = item[\"Question\"]\n",
    "        aliases = item[\"Answer\"].get(\"NormalizedAliases\") or []\n",
    "        gold = aliases[0] if aliases else item[\"Answer\"][\"NormalizedValue\"]\n",
    "        qa_pairs.append({\"question\": question, \"answer\": gold})\n",
    "\n",
    "        # Wiki evidence\n",
    "        for j, d in enumerate(item.get(\"EntityPages\", [])):\n",
    "            add_doc(d, f\"wiki_{d.get('Title','wiki')}\", j)\n",
    "        # Web search evidence\n",
    "        for j, d in enumerate(item.get(\"SearchResults\", [])):\n",
    "            add_doc(d, \"web\", j)\n",
    "\n",
    "    # Drop empties\n",
    "    docs = {k: v for k, v in docs.items() if v.strip()}\n",
    "    print(f\"[load] kept {len(docs)} non-empty docs, {len(qa_pairs)} QA pairs\")\n",
    "    return docs, qa_pairs\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Exact-Match helper\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def exact_match(gold: str, passages: List[str]) -> int:\n",
    "    g = gold.lower()\n",
    "    return int(any(g in p.lower() for p in passages))\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Main\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    # -------- paths & params ------------------------------------------------\n",
    "    TQA_JSON = \"data/triviaqa-unfiltered/unfiltered-web-dev.json\"   # edit if needed\n",
    "    MAX_DOCS = 300\n",
    "    MAX_QA   = 100\n",
    "    TOP_K    = 5\n",
    "\n",
    "    # -------- load ----------------------------------------------------------\n",
    "    docs, qa_pairs = load_triviaqa(TQA_JSON, MAX_DOCS, MAX_QA)\n",
    "    if not docs:\n",
    "        raise RuntimeError(\"No non-empty docs – check dataset path / permissions\")\n",
    "\n",
    "    # -------- choose chunker ------------------------------------------------\n",
    "    chunker = FixedChunker(chunk_size=128, drop_last=False)\n",
    "    # chunker = OverlappingChunker(chunk_size=256, overlap=64)\n",
    "    # chunker = SemanticChunker(chunk_char_limit=1500)\n",
    "\n",
    "    chunk_texts, chunk_to_doc = [], []\n",
    "    for doc_id, txt in docs.items():\n",
    "        for ch in chunker.chunk(txt):\n",
    "            chunk_texts.append(ch)\n",
    "            chunk_to_doc.append(doc_id)\n",
    "\n",
    "    print(f\"[chunk] {chunker.__class__.__name__}: {len(chunk_texts)} chunks \"\n",
    "          f\"(≈{len(chunk_texts)/len(docs):.1f} per doc)\")\n",
    "\n",
    "    # -------- BM25 ----------------------------------------------------------\n",
    "    bm25 = BM25Okapi([c.split() for c in chunk_texts])\n",
    "\n",
    "    # -------- DPR -----------------------------------------------------------\n",
    "    model = SentenceTransformer(\"facebook-dpr-ctx_encoder-multiset-base\")\n",
    "    print(\"[dpr] encoding chunks …\")\n",
    "    ctx_emb = model.encode(\n",
    "        chunk_texts,\n",
    "        batch_size=64,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    index = faiss.IndexFlatIP(ctx_emb.shape[1])\n",
    "    index.add(ctx_emb)\n",
    "    print(f\"[dpr] Faiss index: {index.ntotal} vectors\")\n",
    "\n",
    "    # -------- retrieval loop ------------------------------------------------\n",
    "    bm25_hits = dpr_hits = 0\n",
    "    bm25_lat, dpr_lat = [], []\n",
    "\n",
    "    for qa in tqdm(qa_pairs, desc=\"retrieving\"):\n",
    "        q, gold = qa[\"question\"], qa[\"answer\"]\n",
    "\n",
    "        # BM25\n",
    "        t0 = time.perf_counter()\n",
    "        ids = bm25.get_top_n(q.split(), list(range(len(chunk_texts))), n=TOP_K)\n",
    "        bm25_lat.append((time.perf_counter() - t0) * 1e3)\n",
    "        bm25_hits += exact_match(gold, [chunk_texts[i] for i in ids])\n",
    "\n",
    "        # DPR\n",
    "        t0 = time.perf_counter()\n",
    "        q_emb = model.encode([q], normalize_embeddings=True)\n",
    "        _, idxs = index.search(q_emb, TOP_K)\n",
    "        dpr_lat.append((time.perf_counter() - t0) * 1e3)\n",
    "        dpr_hits += exact_match(gold, [chunk_texts[i] for i in idxs[0]])\n",
    "\n",
    "    # -------- summary -------------------------------------------------------\n",
    "    n = len(qa_pairs)\n",
    "    print(\"\\n=== RESULTS (Exact-Match evidence recall) ===\")\n",
    "    print(f\"BM25 | EM@{TOP_K}: {bm25_hits/n:.3f} | avg latency {np.mean(bm25_lat):.1f} ms\")\n",
    "    print(f\"DPR  | EM@{TOP_K}: {dpr_hits/n:.3f} | avg latency {np.mean(dpr_lat):.1f} ms\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
