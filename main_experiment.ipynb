{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abedc14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TriviaQA dataset from: ../triviaqa-unfiltered/unfiltered-web-dev.json\n",
      "Loaded 531054 documents and 11313 QA pairs from TriviaQA\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def load_triviaqa(dataset_path: str) -> Tuple[Dict[str, str], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Load TriviaQA dataset and return documents and QA pairs.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the TriviaQA JSON file (e.g., 'unfiltered-dev.json')\n",
    "        max_docs: Maximum number of documents to load\n",
    "        max_qa_pairs: Maximum number of QA pairs to load\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (documents dictionary, list of QA pairs)\n",
    "    \"\"\"\n",
    "    print(f\"Loading TriviaQA dataset from: {dataset_path}\")\n",
    "    \n",
    "    # Load the TriviaQA dataset\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)['Data']\n",
    "    \n",
    "    # Extract documents and QA pairs\n",
    "    docs = {}\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "            \n",
    "        question = item['Question']\n",
    "        answers = item['Answer']['NormalizedAliases']\n",
    "        \n",
    "        # Use the first answer as the gold answer\n",
    "        gold_answer = answers[0] if answers else item['Answer']['NormalizedValue']\n",
    "        \n",
    "        qa_pairs.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": gold_answer\n",
    "        })\n",
    "        \n",
    "        # Extract documents from Wikipedia sources\n",
    "        if 'EntityPages' in item:\n",
    "            for j, doc_info in enumerate(item['EntityPages']):\n",
    "                    \n",
    "                doc_id = f\"wiki_{doc_info['Title']}_{j}\"\n",
    "                \n",
    "                # Check if file exists\n",
    "                if 'Filename' in doc_info:\n",
    "                    doc_path = os.path.join(os.path.dirname(dataset_path), doc_info['Filename'])\n",
    "                    if os.path.exists(doc_path):\n",
    "                        try:\n",
    "                            with open(doc_path, 'r', encoding='utf-8') as doc_file:\n",
    "                                doc_content = doc_file.read()\n",
    "                                docs[doc_id] = doc_content\n",
    "                        except:\n",
    "                            # If file can't be read, use the snippet\n",
    "                            docs[doc_id] = doc_info.get('Snippet', '')\n",
    "                    else:\n",
    "                        # If file doesn't exist, use the snippet\n",
    "                        docs[doc_id] = doc_info.get('Snippet', '')\n",
    "                else:\n",
    "                    # If no filename, use the snippet\n",
    "                    docs[doc_id] = doc_info.get('Snippet', '')\n",
    "                    \n",
    "        # Add web documents if available\n",
    "        if 'SearchResults' in item:\n",
    "            for j, doc_info in enumerate(item['SearchResults']):\n",
    "                    \n",
    "                doc_id = f\"web_{j}_{doc_info.get('Title', '')}\"\n",
    "                \n",
    "                if 'Filename' in doc_info:\n",
    "                    doc_path = os.path.join(os.path.dirname(dataset_path), doc_info['Filename'])\n",
    "                    if os.path.exists(doc_path):\n",
    "                        try:\n",
    "                            with open(doc_path, 'r', encoding='utf-8') as doc_file:\n",
    "                                doc_content = doc_file.read()\n",
    "                                docs[doc_id] = doc_content\n",
    "                        except:\n",
    "                            docs[doc_id] = doc_info.get('Snippet', '')\n",
    "                    else:\n",
    "                        docs[doc_id] = doc_info.get('Snippet', '')\n",
    "                else:\n",
    "                    docs[doc_id] = doc_info.get('Snippet', '')\n",
    "    \n",
    "    print(f\"Loaded {len(docs)} documents and {len(qa_pairs)} QA pairs from TriviaQA\")\n",
    "    return docs, qa_pairs \n",
    "\n",
    "documents, qa_pairs = load_triviaqa(\"../triviaqa-unfiltered/unfiltered-web-dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a00d9182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: anyio==4.9.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: appnope==0.1.4 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (0.1.4)\n",
      "Requirement already satisfied: argon2-cffi==23.1.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (21.2.0)\n",
      "Requirement already satisfied: arrow==1.3.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: asttokens==3.0.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: async-lru==2.0.5 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (2.0.5)\n",
      "Requirement already satisfied: attrs==25.3.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (25.3.0)\n",
      "Requirement already satisfied: babel==2.17.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (2.17.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.13.4 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (4.13.4)\n",
      "Requirement already satisfied: bleach==6.2.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (6.2.0)\n",
      "Requirement already satisfied: certifi==2025.4.26 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (2025.4.26)\n",
      "Requirement already satisfied: cffi==1.17.1 in /conda/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (1.17.1)\n",
      "Requirement already satisfied: charset-normalizer==3.4.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (3.4.1)\n",
      "Requirement already satisfied: comm==0.2.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (0.2.2)\n",
      "Requirement already satisfied: debugpy==1.8.14 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (1.8.14)\n",
      "Requirement already satisfied: decorator==5.2.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (5.2.1)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (0.7.1)\n",
      "Requirement already satisfied: exceptiongroup==1.2.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (1.2.2)\n",
      "Requirement already satisfied: executing==2.2.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 20)) (2.2.0)\n",
      "Requirement already satisfied: fastjsonschema==2.21.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (2.21.1)\n",
      "Requirement already satisfied: filelock==3.18.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (3.18.0)\n",
      "Requirement already satisfied: fqdn==1.5.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 23)) (1.5.1)\n",
      "Requirement already satisfied: fsspec==2025.3.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 24)) (2025.3.2)\n",
      "Requirement already satisfied: h11==0.16.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 25)) (0.16.0)\n",
      "Requirement already satisfied: httpcore==1.0.9 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 26)) (1.0.9)\n",
      "Requirement already satisfied: httpx==0.28.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 27)) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub==0.30.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 28)) (0.30.2)\n",
      "Requirement already satisfied: idna==3.10 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 29)) (3.10)\n",
      "Requirement already satisfied: importlib_metadata==8.7.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 30)) (8.7.0)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 31)) (6.29.5)\n",
      "Requirement already satisfied: ipython==8.18.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 32)) (8.18.1)\n",
      "Requirement already satisfied: isoduration==20.11.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 33)) (20.11.0)\n",
      "Requirement already satisfied: jedi==0.19.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 34)) (0.19.2)\n",
      "Requirement already satisfied: Jinja2==3.1.6 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 35)) (3.1.6)\n",
      "Requirement already satisfied: joblib==1.4.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 36)) (1.4.2)\n",
      "Requirement already satisfied: json5==0.12.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 37)) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer==3.0.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 38)) (3.0.0)\n",
      "Requirement already satisfied: jsonschema==4.23.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 39)) (4.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications==2025.4.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 40)) (2025.4.1)\n",
      "Requirement already satisfied: jupyter-events==0.12.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 41)) (0.12.0)\n",
      "Requirement already satisfied: jupyter-lsp==2.2.5 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 42)) (2.2.5)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 43)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 44)) (5.7.2)\n",
      "Requirement already satisfied: jupyter_server==2.15.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 45)) (2.15.0)\n",
      "Requirement already satisfied: jupyter_server_terminals==0.5.3 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 46)) (0.5.3)\n",
      "Requirement already satisfied: jupyterlab==4.4.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 47)) (4.4.1)\n",
      "Requirement already satisfied: jupyterlab_pygments==0.3.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 48)) (0.3.0)\n",
      "Requirement already satisfied: jupyterlab_server==2.27.3 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 49)) (2.27.3)\n",
      "Requirement already satisfied: MarkupSafe==3.0.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 50)) (3.0.2)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 51)) (0.1.7)\n",
      "Requirement already satisfied: mistune==3.1.3 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 52)) (3.1.3)\n",
      "Requirement already satisfied: mpmath==1.3.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 53)) (1.3.0)\n",
      "Requirement already satisfied: nbclient==0.10.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 54)) (0.10.2)\n",
      "Requirement already satisfied: nbconvert==7.16.6 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 55)) (7.16.6)\n",
      "Requirement already satisfied: nbformat==5.10.4 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 56)) (5.10.4)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 57)) (1.6.0)\n",
      "Requirement already satisfied: networkx==3.2.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 58)) (3.2.1)\n",
      "Requirement already satisfied: notebook==7.4.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 59)) (7.4.1)\n",
      "Requirement already satisfied: notebook_shim==0.2.4 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 60)) (0.2.4)\n",
      "Requirement already satisfied: numpy==2.0.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 61)) (2.0.2)\n",
      "Requirement already satisfied: overrides==7.7.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 62)) (7.7.0)\n",
      "Requirement already satisfied: packaging==25.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 63)) (25.0)\n",
      "Requirement already satisfied: pandocfilters==1.5.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 64)) (1.5.1)\n",
      "Requirement already satisfied: parso==0.8.4 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 65)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 66)) (4.9.0)\n",
      "Requirement already satisfied: pillow==11.2.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 67)) (11.2.1)\n",
      "Requirement already satisfied: platformdirs==4.3.7 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 68)) (4.3.7)\n",
      "Requirement already satisfied: prometheus_client==0.21.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 69)) (0.21.1)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.51 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 70)) (3.0.51)\n",
      "Requirement already satisfied: psutil==7.0.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 71)) (7.0.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 72)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 73)) (0.2.3)\n",
      "Requirement already satisfied: pycparser==2.22 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 74)) (2.22)\n",
      "Requirement already satisfied: Pygments==2.19.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 75)) (2.19.1)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 76)) (2.9.0.post0)\n",
      "Requirement already satisfied: python-json-logger==3.3.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 77)) (3.3.0)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 78)) (6.0.2)\n",
      "Requirement already satisfied: pyzmq==26.4.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 79)) (26.4.0)\n",
      "Requirement already satisfied: rank-bm25==0.2.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 80)) (0.2.2)\n",
      "Requirement already satisfied: referencing==0.36.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 81)) (0.36.2)\n",
      "Requirement already satisfied: regex==2024.11.6 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 82)) (2024.11.6)\n",
      "Requirement already satisfied: requests==2.32.3 in /conda/lib/python3.12/site-packages (from -r requirements.txt (line 83)) (2.32.3)\n",
      "Requirement already satisfied: rfc3339-validator==0.1.4 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 84)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator==0.1.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 85)) (0.1.1)\n",
      "Requirement already satisfied: rpds-py==0.24.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 86)) (0.24.0)\n",
      "Requirement already satisfied: safetensors==0.5.3 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 87)) (0.5.3)\n",
      "Requirement already satisfied: scikit-learn==1.6.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 88)) (1.6.1)\n",
      "Requirement already satisfied: scipy==1.13.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 89)) (1.13.1)\n",
      "Requirement already satisfied: Send2Trash==1.8.3 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 90)) (1.8.3)\n",
      "Requirement already satisfied: sentence-transformers==4.1.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 91)) (4.1.0)\n",
      "Requirement already satisfied: six==1.17.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 92)) (1.17.0)\n",
      "Requirement already satisfied: sniffio==1.3.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 93)) (1.3.1)\n",
      "Requirement already satisfied: soupsieve==2.7 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 94)) (2.7)\n",
      "Requirement already satisfied: stack-data==0.6.3 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 95)) (0.6.3)\n",
      "Requirement already satisfied: sympy==1.14.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 96)) (1.14.0)\n",
      "Requirement already satisfied: terminado==0.18.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 97)) (0.18.1)\n",
      "Requirement already satisfied: threadpoolctl==3.6.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 98)) (3.6.0)\n",
      "Requirement already satisfied: tinycss2==1.4.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 99)) (1.4.0)\n",
      "Requirement already satisfied: tokenizers==0.21.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 100)) (0.21.1)\n",
      "Requirement already satisfied: tomli==2.2.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 101)) (2.2.1)\n",
      "Requirement already satisfied: torch==2.7.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 102)) (2.7.0)\n",
      "Requirement already satisfied: tornado==6.4.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 103)) (6.4.2)\n",
      "Requirement already satisfied: tqdm==4.67.1 in /conda/lib/python3.12/site-packages (from -r requirements.txt (line 104)) (4.67.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 105)) (5.14.3)\n",
      "Requirement already satisfied: transformers==4.51.3 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 106)) (4.51.3)\n",
      "Requirement already satisfied: types-python-dateutil==2.9.0.20241206 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 107)) (2.9.0.20241206)\n",
      "Requirement already satisfied: typing_extensions==4.13.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 108)) (4.13.2)\n",
      "Requirement already satisfied: uri-template==1.3.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 109)) (1.3.0)\n",
      "Requirement already satisfied: urllib3==2.4.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 110)) (2.4.0)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 111)) (0.2.13)\n",
      "Requirement already satisfied: webcolors==24.11.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 112)) (24.11.1)\n",
      "Requirement already satisfied: webencodings==0.5.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 113)) (0.5.1)\n",
      "Requirement already satisfied: websocket-client==1.8.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 114)) (1.8.0)\n",
      "Requirement already satisfied: zipp==3.21.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from -r requirements.txt (line 115)) (3.21.0)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in /conda/lib/python3.12/site-packages (from jupyterlab==4.4.1->-r requirements.txt (line 47)) (75.8.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/gdurand2/.local/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 102)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/gdurand2/.local/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 102)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/gdurand2/.local/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 102)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/gdurand2/.local/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 102)) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/gdurand2/.local/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 102)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/gdurand2/.local/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 102)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/gdurand2/.local/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 102)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 102)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 102)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/gdurand2/.local/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 102)) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/gdurand2/.local/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 102)) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/gdurand2/.local/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 102)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/gdurand2/.local/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 102)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/gdurand2/.local/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 102)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/gdurand2/.local/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 102)) (3.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a548d4c5c0c35d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T00:16:42.197109Z",
     "start_time": "2025-04-28T00:16:37.510365Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbm25_retriever\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BM25Retriever\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolbert_retriever\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColBERTRetriever\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdense_retriever\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DPRRetriever\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/598-ds/598_project/retrievers/bm25_retriever.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBM25Retriever\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from retrievers.bm25_retriever import BM25Retriever\n",
    "from retrievers.colbert_retriever import ColBERTRetriever\n",
    "from retrievers.dense_retriever import DPRRetriever\n",
    "from retrievers.hybrid_retriever import HybridRetriever\n",
    "\n",
    "\n",
    "# Initialize the Retrievers with the documents\n",
    "bm25_retriever = BM25Retriever(documents)\n",
    "dense_retriever = DPRRetriever(documents)\n",
    "colbert_retriever = ColBERTRetriever(documents)\n",
    "hybrid_retriever = HybridRetriever(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a583a68580938af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T00:16:44.395789Z",
     "start_time": "2025-04-28T00:16:43.530148Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k retrieved documents (BM25): ['The sky is pink', 'The sky is blue.']\n",
      "Top-k retrieved documents (DPR): ['The sky is blue.', 'The sky is pink']\n",
      "Top-k retrieved documents (ColBERT): ['The sky is blue.', 'The sky is pink']\n",
      "Top-k retrieved documents (Hybrid): ['The sky is blue.', 'The sky is pink']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Retriever with the documents\n",
    "query = \"What is the color of the sky?\"\n",
    "\n",
    "# Retrieve the top 2 most relevant documents based on the query\n",
    "top_k = 2\n",
    "bm25_retrieved_docs = bm25_retriever.retrieve(query, top_k=top_k)\n",
    "dpr_retrieved_docs = dense_retriever.retrieve(query, top_k=top_k)\n",
    "colbert_retrieved_docs = colbert_retriever.retrieve(query, top_k=top_k)\n",
    "hybrid_retrieved_docs = hybrid_retriever.retrieve(query, top_k=top_k)\n",
    "\n",
    "# Output the retrieved documents\"\n",
    "print(\"Top-k retrieved documents (BM25):\", bm25_retrieved_docs)\n",
    "print(\"Top-k retrieved documents (DPR):\", dpr_retrieved_docs)\n",
    "print(\"Top-k retrieved documents (ColBERT):\", colbert_retrieved_docs)\n",
    "print(\"Top-k retrieved documents (Hybrid):\", hybrid_retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd4fbc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed-length chunks (20 tokens each):\n",
      "  01. 'The sky appears blue because molecules in the air scatter blue light from the sun more than they scatter red'\n",
      "  02. 'light. This phenomenon is called Rayleigh scattering. When the sun is lower in the sky, the light has to pass'\n",
      "  03. 'through more atmosphere, so more blue and green light is scattered away, leaving the reds and oranges we see at'\n",
      "\n",
      "Overlapping chunks (24 tokens, 8-token overlap):\n",
      "  01. 'The sky appears blue because molecules in the air scatter blue light from the sun more than they scatter red light. This phenomenon is'\n",
      "  02. 'than they scatter red light. This phenomenon is called Rayleigh scattering. When the sun is lower in the sky, the light has to pass'\n",
      "  03. 'in the sky, the light has to pass through more atmosphere, so more blue and green light is scattered away, leaving the reds and'\n",
      "  04. 'light is scattered away, leaving the reds and oranges we see at sunrise and sunset.'\n",
      "\n",
      "Semantic chunks (~120 chars, sentence-aware):\n",
      "  01. 'The sky appears blue because molecules in the air scatter blue light from the sun more than they scatter red light.'\n",
      "  02. 'This phenomenon is called Rayleigh scattering.'\n",
      "  03. 'When the sun is lower in the sky, the light has to pass through more atmosphere, so more blue and green light is scattered away, leaving the reds and oranges we see at sunrise and sunset.'\n"
     ]
    }
   ],
   "source": [
    "# --- Example text ----------------------------------------------------------\n",
    "doc = (\n",
    "    \"The sky appears blue because molecules in the air scatter blue light from \"\n",
    "    \"the sun more than they scatter red light. This phenomenon is called Rayleigh \"\n",
    "    \"scattering. When the sun is lower in the sky, the light has to pass through \"\n",
    "    \"more atmosphere, so more blue and green light is scattered away, leaving the \"\n",
    "    \"reds and oranges we see at sunrise and sunset.\"\n",
    ")\n",
    "\n",
    "# --- Import the chunkers ---------------------------------------------------\n",
    "from chunkers import FixedChunker, OverlappingChunker, SemanticChunker\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Fixed-length chunking: split every 20 tokens (drop tail if < 20 tokens)\n",
    "fixed_chunker = FixedChunker(chunk_size=20, drop_last=True)\n",
    "fixed_chunks = fixed_chunker.chunk(doc)\n",
    "\n",
    "# 2. Overlapping windows: 24-token windows with 8-token overlap\n",
    "overlap_chunker = OverlappingChunker(chunk_size=24, overlap=8, drop_last=False)\n",
    "overlap_chunks = overlap_chunker.chunk(doc)\n",
    "\n",
    "# 3. Semantic packing: keep whole sentences, ~120 characters per chunk\n",
    "semantic_chunker = SemanticChunker(chunk_char_limit=120)\n",
    "semantic_chunks = semantic_chunker.chunk(doc)\n",
    "\n",
    "# --- Inspect the output ----------------------------------------------------\n",
    "print(\"Fixed-length chunks (20 tokens each):\")\n",
    "for i, c in enumerate(fixed_chunks, 1):\n",
    "    print(f\"  {i:02d}. {c!r}\")\n",
    "\n",
    "print(\"\\nOverlapping chunks (24 tokens, 8-token overlap):\")\n",
    "for i, c in enumerate(overlap_chunks, 1):\n",
    "    print(f\"  {i:02d}. {c!r}\")\n",
    "\n",
    "print(\"\\nSemantic chunks (~120 chars, sentence-aware):\")\n",
    "for i, c in enumerate(semantic_chunks, 1):\n",
    "    print(f\"  {i:02d}. {c!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39985b2e30f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 0.  Imports & one-time setup\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict\n",
    "import json, time\n",
    "\n",
    "# local modules\n",
    "from chunkers import FixedChunker, OverlappingChunker, SemanticChunker\n",
    "from retrievers import bm25_retriever, dense_retriever, hybrid_retriever\n",
    "from retrievers.dense_retriever import DPRRetriever  # example alias\n",
    "from utils.evaluation import exact_match, f1_score            # already in your repo\n",
    "from utils.timing import Timer                                 # already in your repo\n",
    "\n",
    "# external (install if missing)\n",
    "#   pip install rank-bm25 sentence-transformers faiss-cpu\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  Load a toy corpus\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "DOCS: Dict[str, str] = {\n",
    "    \"doc1\": Path(\"data/doc1.txt\").read_text(encoding=\"utf-8\"),\n",
    "    \"doc2\": Path(\"data/doc2.txt\").read_text(encoding=\"utf-8\"),\n",
    "    \"doc3\": Path(\"data/doc3.txt\").read_text(encoding=\"utf-8\"),\n",
    "}\n",
    "print(f\"Loaded {len(DOCS)} raw documents\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  Choose **one** chunker recipe for this run\n",
    "#     (Swap these objects in a loop if you’re doing a grid-search)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "chunker = FixedChunker(chunk_size=128, drop_last=False)\n",
    "# chunker = OverlappingChunker(chunk_size=256, overlap=64)\n",
    "# chunker = SemanticChunker(chunk_char_limit=1500)\n",
    "\n",
    "# 2-b.  Chunk every document → corpus_chunks[id] = [chunk0, …]\n",
    "corpus_chunks: Dict[str, List[str]] = {\n",
    "    doc_id: chunker.chunk(txt) for doc_id, txt in DOCS.items()\n",
    "}\n",
    "flat_chunks = [c for chs in corpus_chunks.values() for c in chs]\n",
    "print(\n",
    "    f\"Chunked into {len(flat_chunks):,} total passages \"\n",
    "    f\"(avg {len(flat_chunks)/len(DOCS):.1f} per doc)\"\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  Build the four retrieval back-ends on **exactly the same chunk set**\n",
    "#     The dense & hybrid examples assume a SentenceTransformer-based DPR.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3-a. BM25 (sparse)\n",
    "bm25_index = BM25Okapi([c.split() for c in flat_chunks])\n",
    "\n",
    "# 3-b. Dense Passage Retriever\n",
    "dpr_model = SentenceTransformer(\"facebook-dpr-ctx_encoder-multiset-base\")\n",
    "dpr_index = dense_retriever.build_faiss_index(flat_chunks, dpr_model)  # your util\n",
    "\n",
    "# 3-c. ColBERT (late interaction)  ← placeholder call\n",
    "colbert_retriever = dense_retriever.ColBERTIndexer().fit(flat_chunks)\n",
    "\n",
    "# 3-d. Hybrid (= BM25 + DPR scores)\n",
    "hybrid = hybrid_retriever.Hybrid(bm25_index, dpr_index, alpha=0.4)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4.  Query loop + simple QA evaluation\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "qa_pairs = json.loads(Path(\"data/dev_qas.json\").read_text())[:50]  # small dev slice\n",
    "K = 5\n",
    "\n",
    "results_by_system = defaultdict(list)\n",
    "\n",
    "for q in qa_pairs:\n",
    "    query, gold = q[\"question\"], q[\"answer\"]  # gold answer string\n",
    "\n",
    "    with Timer() as t:\n",
    "        top_chunks = bm25_index.get_top_n(query.split(), flat_chunks, n=K)\n",
    "    results_by_system[\"bm25\"].append(\n",
    "        {\"latency_ms\": t.ms, \"em\": exact_match(gold, top_chunks)}\n",
    "    )\n",
    "\n",
    "    with Timer() as t:\n",
    "        top_chunks = dense_retriever.search_faiss(query, dpr_index, dpr_model, top_k=K)\n",
    "    results_by_system[\"dpr\"].append(\n",
    "        {\"latency_ms\": t.ms, \"em\": exact_match(gold, top_chunks)}\n",
    "    )\n",
    "\n",
    "    with Timer() as t:\n",
    "        top_chunks = colbert_retriever.search(query, top_k=K)\n",
    "    results_by_system[\"colbert\"].append(\n",
    "        {\"latency_ms\": t.ms, \"em\": exact_match(gold, top_chunks)}\n",
    "    )\n",
    "\n",
    "    with Timer() as t:\n",
    "        top_chunks = hybrid.search(query, top_k=K)\n",
    "    results_by_system[\"hybrid\"].append(\n",
    "        {\"latency_ms\": t.ms, \"em\": exact_match(gold, top_chunks)}\n",
    "    )\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5.  Aggregate & display – how *this* chunker impacted each retriever\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def summarise(rows):\n",
    "    return {\n",
    "        \"EM\": sum(r[\"em\"] for r in rows) / len(rows),\n",
    "        \"Latency (ms)\": sum(r[\"latency_ms\"] for r in rows) / len(rows),\n",
    "    }\n",
    "\n",
    "summary = {name: summarise(rows) for name, rows in results_by_system.items()}\n",
    "print(\"\\n===  Chunker:\", chunker.__class__.__name__, \" ===\")\n",
    "for system, metrics in summary.items():\n",
    "    print(f\"{system:7s} | EM={metrics['EM']:.3f} | Latency≈{metrics['Latency (ms)']:.1f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4bb226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load] TriviaQA from data/triviaqa-unfiltered/unfiltered-web-dev.json\n",
      "[load] kept 250 non-empty docs, 100 QA pairs\n",
      "[chunk] FixedChunker: 250 chunks (≈1.0 per doc)\n",
      "[dpr] encoding chunks …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:01<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dpr] Faiss index: 250 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "retrieving: 100%|██████████| 100/100 [00:04<00:00, 22.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESULTS (Exact-Match evidence recall) ===\n",
      "BM25 | EM@5: 0.010 | avg latency 0.8 ms\n",
      "DPR  | EM@5: 0.000 | avg latency 43.2 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Minimal Retrieval-only demo on TriviaQA-unfiltered.\n",
    "\n",
    "Dependencies:\n",
    "  pip install rank-bm25 sentence-transformers faiss-cpu tqdm\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, gzip, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Retrieval libs\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss                           # CPU version is OK\n",
    "\n",
    "# Your chunkers (put the three modules in rag_pipeline/chunkers/)\n",
    "from chunkers import FixedChunker, OverlappingChunker, SemanticChunker\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 1. TriviaQA loader (improved)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def load_triviaqa(\n",
    "    dataset_path: str,\n",
    "    max_docs: int = 300,\n",
    "    max_qa_pairs: int = 100,\n",
    ") -> Tuple[Dict[str, str], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        docs   : {doc_id: raw_text}\n",
    "        qa_pairs: [{\"question\": str, \"answer\": str}, …]\n",
    "    \"\"\"\n",
    "    print(f\"[load] TriviaQA from {dataset_path}\")\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)[\"Data\"]\n",
    "\n",
    "    root = Path(dataset_path).parent\n",
    "    docs: Dict[str, str] = {}\n",
    "    qa_pairs: List[Dict] = []\n",
    "\n",
    "    def read_evidence(file_path: Path) -> str:\n",
    "        if not file_path.exists():\n",
    "            return \"\"\n",
    "        # Many evidence files are .gz\n",
    "        try:\n",
    "            if file_path.suffix == \".gz\":\n",
    "                with gzip.open(file_path, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as g:\n",
    "                    return g.read()\n",
    "            return file_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def add_doc(doc_info, prefix: str, j: int):\n",
    "        if len(docs) >= max_docs:\n",
    "            return\n",
    "        doc_id = f\"{prefix}_{j}\"\n",
    "        txt = \"\"\n",
    "        if doc_info.get(\"Filename\"):\n",
    "            txt = read_evidence(root / doc_info[\"Filename\"])\n",
    "        # Fallbacks\n",
    "        txt = txt or doc_info.get(\"Snippet\", \"\") or doc_info.get(\"Title\", \"\") or doc_info.get(\"Url\", \"\")\n",
    "        docs[doc_id] = txt\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        if i >= max_qa_pairs:\n",
    "            break\n",
    "\n",
    "        # QA\n",
    "        question = item[\"Question\"]\n",
    "        aliases = item[\"Answer\"].get(\"NormalizedAliases\") or []\n",
    "        gold = aliases[0] if aliases else item[\"Answer\"][\"NormalizedValue\"]\n",
    "        qa_pairs.append({\"question\": question, \"answer\": gold})\n",
    "\n",
    "        # Wiki evidence\n",
    "        for j, d in enumerate(item.get(\"EntityPages\", [])):\n",
    "            add_doc(d, f\"wiki_{d.get('Title','wiki')}\", j)\n",
    "        # Web search evidence\n",
    "        for j, d in enumerate(item.get(\"SearchResults\", [])):\n",
    "            add_doc(d, \"web\", j)\n",
    "\n",
    "    # Drop empties\n",
    "    docs = {k: v for k, v in docs.items() if v.strip()}\n",
    "    print(f\"[load] kept {len(docs)} non-empty docs, {len(qa_pairs)} QA pairs\")\n",
    "    return docs, qa_pairs\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Exact-Match helper\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def exact_match(gold: str, passages: List[str]) -> int:\n",
    "    g = gold.lower()\n",
    "    return int(any(g in p.lower() for p in passages))\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Main\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    # -------- paths & params ------------------------------------------------\n",
    "    TQA_JSON = \"data/triviaqa-unfiltered/unfiltered-web-dev.json\"   # edit if needed\n",
    "    MAX_DOCS = 300\n",
    "    MAX_QA   = 100\n",
    "    TOP_K    = 5\n",
    "\n",
    "    # -------- load ----------------------------------------------------------\n",
    "    docs, qa_pairs = load_triviaqa(TQA_JSON, MAX_DOCS, MAX_QA)\n",
    "    if not docs:\n",
    "        raise RuntimeError(\"No non-empty docs – check dataset path / permissions\")\n",
    "\n",
    "    # -------- choose chunker ------------------------------------------------\n",
    "    chunker = FixedChunker(chunk_size=128, drop_last=False)\n",
    "    # chunker = OverlappingChunker(chunk_size=256, overlap=64)\n",
    "    # chunker = SemanticChunker(chunk_char_limit=1500)\n",
    "\n",
    "    chunk_texts, chunk_to_doc = [], []\n",
    "    for doc_id, txt in docs.items():\n",
    "        for ch in chunker.chunk(txt):\n",
    "            chunk_texts.append(ch)\n",
    "            chunk_to_doc.append(doc_id)\n",
    "\n",
    "    print(f\"[chunk] {chunker.__class__.__name__}: {len(chunk_texts)} chunks \"\n",
    "          f\"(≈{len(chunk_texts)/len(docs):.1f} per doc)\")\n",
    "\n",
    "    # -------- BM25 ----------------------------------------------------------\n",
    "    bm25 = BM25Okapi([c.split() for c in chunk_texts])\n",
    "\n",
    "    # -------- DPR -----------------------------------------------------------\n",
    "    model = SentenceTransformer(\"facebook-dpr-ctx_encoder-multiset-base\")\n",
    "    print(\"[dpr] encoding chunks …\")\n",
    "    ctx_emb = model.encode(\n",
    "        chunk_texts,\n",
    "        batch_size=64,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    index = faiss.IndexFlatIP(ctx_emb.shape[1])\n",
    "    index.add(ctx_emb)\n",
    "    print(f\"[dpr] Faiss index: {index.ntotal} vectors\")\n",
    "\n",
    "    # -------- retrieval loop ------------------------------------------------\n",
    "    bm25_hits = dpr_hits = 0\n",
    "    bm25_lat, dpr_lat = [], []\n",
    "\n",
    "    for qa in tqdm(qa_pairs, desc=\"retrieving\"):\n",
    "        q, gold = qa[\"question\"], qa[\"answer\"]\n",
    "\n",
    "        # BM25\n",
    "        t0 = time.perf_counter()\n",
    "        ids = bm25.get_top_n(q.split(), list(range(len(chunk_texts))), n=TOP_K)\n",
    "        bm25_lat.append((time.perf_counter() - t0) * 1e3)\n",
    "        bm25_hits += exact_match(gold, [chunk_texts[i] for i in ids])\n",
    "\n",
    "        # DPR\n",
    "        t0 = time.perf_counter()\n",
    "        q_emb = model.encode([q], normalize_embeddings=True)\n",
    "        _, idxs = index.search(q_emb, TOP_K)\n",
    "        dpr_lat.append((time.perf_counter() - t0) * 1e3)\n",
    "        dpr_hits += exact_match(gold, [chunk_texts[i] for i in idxs[0]])\n",
    "\n",
    "    # -------- summary -------------------------------------------------------\n",
    "    n = len(qa_pairs)\n",
    "    print(\"\\n=== RESULTS (Exact-Match evidence recall) ===\")\n",
    "    print(f\"BM25 | EM@{TOP_K}: {bm25_hits/n:.3f} | avg latency {np.mean(bm25_lat):.1f} ms\")\n",
    "    print(f\"DPR  | EM@{TOP_K}: {dpr_hits/n:.3f} | avg latency {np.mean(dpr_lat):.1f} ms\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
