{
 "cells": [
  {
   "cell_type": "code",
   "id": "735019c6165dae6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:27:22.082649Z",
     "start_time": "2025-05-04T22:27:22.019987Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def load_triviaqa(dataset_path: str) -> Tuple[Dict[str, str], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Load TriviaQA dataset and return documents and QA pairs.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the TriviaQA JSON file (e.g., 'unfiltered-dev.json')\n",
    "        max_docs: Maximum number of documents to load\n",
    "        max_qa_pairs: Maximum number of QA pairs to load\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (documents dictionary, list of QA pairs)\n",
    "    \"\"\"\n",
    "    print(f\"Loading TriviaQA dataset from: {dataset_path}\")\n",
    "    \n",
    "    # Load the TriviaQA dataset\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)['Data']\n",
    "    \n",
    "    # Extract documents and QA pairs\n",
    "    docs = {}\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        # if len(docs)>200:\n",
    "        #     break\n",
    "        question = item['Question']\n",
    "        answers = item['Answer']['NormalizedAliases']\n",
    "        \n",
    "        # Use the first answer as the gold answer\n",
    "        gold_answer = answers if answers else list(item['Answer']['NormalizedValue'])\n",
    "        \n",
    "        qa_pairs.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": gold_answer\n",
    "        })\n",
    "        \n",
    "        # Extract documents from Wikipedia sources\n",
    "        if 'EntityPages' in item:\n",
    "            for j, doc_info in enumerate(item['EntityPages']):\n",
    "                    \n",
    "                doc_id = f\"wiki_{doc_info['Title']}_{j}\"\n",
    "                \n",
    "                # Check if file exists\n",
    "                if 'Filename' in doc_info:\n",
    "                    doc_path = os.path.join(os.path.dirname(\"data/wikipedia\"), doc_info['Filename'])\n",
    "                    if os.path.exists(doc_path):\n",
    "                        try:\n",
    "                            with open(doc_path, 'r', encoding='utf-8') as doc_file:\n",
    "                                doc_content = doc_file.read()\n",
    "                                docs[doc_id] = doc_content\n",
    "\n",
    "                        except:\n",
    "                            # If file can't be read, use the snippet\n",
    "                            docs[doc_id] = doc_info.get('Snippet', '')\n",
    "                    else:\n",
    "                        # If file doesn't exist, use the snippet\n",
    "                        docs[doc_id] = doc_info.get('Snippet', '')\n",
    "                else:\n",
    "                    # If no filename, use the snippet\n",
    "                    docs[doc_id] = doc_info.get('Snippet', '')\n",
    "                    \n",
    "        # Add web documents if available\n",
    "        if 'SearchResults' in item:\n",
    "            for j, doc_info in enumerate(item['SearchResults']):\n",
    "                    \n",
    "                doc_id = f\"web_{j}_{doc_info.get('Title', '')}\"\n",
    "                \n",
    "                if 'Filename' in doc_info:\n",
    "                    doc_path = os.path.join(\"data/web\", doc_info['Filename'])\n",
    "                    if os.path.exists(doc_path):\n",
    "                        try:\n",
    "                            with open(doc_path, 'r', encoding='utf-8') as doc_file:\n",
    "                                doc_content = doc_file.read()\n",
    "                                docs[doc_id] = doc_content\n",
    "                        except:\n",
    "                            docs[doc_id] = doc_info.get('Snippet', '')\n",
    "                    else:\n",
    "                        docs[doc_id] = doc_info.get('Snippet', '')\n",
    "                else:\n",
    "                    docs[doc_id] = doc_info.get('Snippet', '')\n",
    "    \n",
    "    print(f\"Loaded {len(docs)} documents and {len(qa_pairs)} QA pairs from TriviaQA\")\n",
    "    return docs, qa_pairs\n",
    "\n",
    "documents_dict, qa_pairs = load_triviaqa(\"triviaqa-unfiltered/verfied-web-dev.json\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TriviaQA dataset from: triviaqa-unfiltered/verfied-web-dev.json\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'triviaqa-unfiltered/verfied-web-dev.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 91\u001B[0m\n\u001B[1;32m     88\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoaded \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(docs)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m documents and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(qa_pairs)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m QA pairs from TriviaQA\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     89\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m docs, qa_pairs\n\u001B[0;32m---> 91\u001B[0m documents_dict, qa_pairs \u001B[38;5;241m=\u001B[39m load_triviaqa(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtriviaqa-unfiltered/verfied-web-dev.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[2], line 22\u001B[0m, in \u001B[0;36mload_triviaqa\u001B[0;34m(dataset_path)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading TriviaQA dataset from: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Load the TriviaQA dataset\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(dataset_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     23\u001B[0m     data \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mData\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Extract documents and QA pairs\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    322\u001B[0m     )\n\u001B[0;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'triviaqa-unfiltered/verfied-web-dev.json'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "def benchmark_retrieval(retriever, query, top_k):\n",
    "    start = time.time()\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    end = time.time()\n",
    "    latency = end - start\n",
    "    return results, latency\n",
    "\n",
    "def match_exists(retrieved_docs, answers):\n",
    "    for doc in retrieved_docs:\n",
    "        if any(ans.lower() in doc.lower() for ans in answers):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def get_memory_usage_mb():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "def compute_precision_recall_f1(retrieved_docs, answers):\n",
    "    retrieved_relevant = sum(1 for doc in retrieved_docs if any(ans.lower() in doc.lower() for ans in answers))\n",
    "    total_retrieved = len(retrieved_docs)\n",
    "    total_relevant = len(answers)  # This assumes each answer is unique and important\n",
    "\n",
    "    precision = retrieved_relevant / total_retrieved if total_retrieved > 0 else 0\n",
    "    recall = retrieved_relevant / total_relevant if total_relevant > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def benchmark_all(strategies, documents, qas):\n",
    "    results = []\n",
    "    overall_retriever_results = []\n",
    "\n",
    "    for chunker_name, chunker in strategies[\"chunkers\"].items():\n",
    "        chunks = []\n",
    "\n",
    "        # Chunking Latency\n",
    "        chunk_start_time = time.time()\n",
    "        for doc in documents:\n",
    "            chunks.extend(chunker.chunk(doc))\n",
    "        chunk_end_time = time.time()\n",
    "        chunking_latency = chunk_end_time - chunk_start_time\n",
    "\n",
    "        for retriever_name, retriever_class in strategies[\"retrievers\"].items():\n",
    "            retriever_initialize_start_time = time.time()\n",
    "            retriever = retriever_class(chunks)\n",
    "            retriever_initialize_end_time = time.time()\n",
    "            retriever_initialize_latency = retriever_initialize_end_time - retriever_initialize_start_time\n",
    "\n",
    "            memory_usage = get_memory_usage_mb()\n",
    "\n",
    "            for top_k in strategies[\"top_k\"]:\n",
    "                total_latency = 0\n",
    "                total_precision = 0\n",
    "                total_recall = 0\n",
    "                total_f1 = 0\n",
    "                correct_docs = 0\n",
    "\n",
    "                for qa in qas:\n",
    "                    query, answers = qa[\"question\"], qa[\"answer\"]\n",
    "                    retrieved_docs, latency = benchmark_retrieval(retriever, query, top_k)\n",
    "\n",
    "                    precision, recall, f1 = compute_precision_recall_f1(retrieved_docs, answers)\n",
    "                    does_match = 1 if precision > 0 else 0\n",
    "\n",
    "                    results.append({\n",
    "                        \"query\": query,\n",
    "                        \"chunker\": chunker_name,\n",
    "                        \"retriever\": retriever_name,\n",
    "                        \"top_k\": top_k,\n",
    "                        \"latency\": latency,\n",
    "                        \"results\": retrieved_docs,\n",
    "                        \"answers\": answers,\n",
    "                        \"does_match\": does_match,\n",
    "                        \"precision\": precision,\n",
    "                        \"recall\": recall,\n",
    "                        \"f1\": f1\n",
    "                    })\n",
    "\n",
    "                    total_latency += latency\n",
    "                    total_precision += precision\n",
    "                    total_recall += recall\n",
    "                    total_f1 += f1\n",
    "                    correct_docs += does_match\n",
    "\n",
    "                num_qas = len(qas)\n",
    "                overall_result = {\n",
    "                    \"retriever\": retriever_name,\n",
    "                    \"chunker\": chunker_name,\n",
    "                    \"top_k\": top_k,\n",
    "                    \"initialize_latency\": retriever_initialize_latency,\n",
    "                    \"retrieve_latency\": total_latency / num_qas,\n",
    "                    \"chunking_latency\": chunking_latency,\n",
    "                    \"accuracy\": correct_docs / num_qas * 100,\n",
    "                    \"avg_precision\": total_precision / num_qas,\n",
    "                    \"avg_recall\": total_recall / num_qas,\n",
    "                    \"avg_f1\": total_f1 / num_qas,\n",
    "                    \"memory_MB\": memory_usage\n",
    "                }\n",
    "\n",
    "                print(overall_result)\n",
    "                overall_retriever_results.append(overall_result)\n",
    "\n",
    "    return results, overall_retriever_results\n",
    "\n"
   ],
   "id": "abedc14f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a00d9182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T07:12:37.177105Z",
     "start_time": "2025-05-01T06:54:09.718353Z"
    }
   },
   "source": [
    "from chunkers import FixedChunker, OverlappingChunker, SemanticChunker\n",
    "from retrievers.bm25_retriever import BM25Retriever\n",
    "from retrievers.dense_retriever import DPRRetriever\n",
    "from retrievers.colbert_retriever import ColBERTRetriever\n",
    "from retrievers.hybrid_retriever import HybridRetriever\n",
    "from retrievers.tf_idf_retriever import TFIDFRetriever\n",
    "\n",
    "\n",
    "documents = list(documents_dict.values())\n",
    "top_k = 5\n",
    "\n",
    "strategies = {\n",
    "    \"chunkers\": {\n",
    "        \"fixed_128\": FixedChunker(chunk_size=128, drop_last=False),\n",
    "        \"fixed_256\": FixedChunker(chunk_size=256, drop_last=False),\n",
    "        \"fixed_512\": FixedChunker(chunk_size=512, drop_last=False),\n",
    "        \"overlapping_256\": OverlappingChunker(chunk_size=2048, overlap=256, drop_last=False),\n",
    "        \"overlapping_512\": OverlappingChunker(chunk_size=2048, overlap=512, drop_last=False),\n",
    "        \"overlapping_1042\": OverlappingChunker(chunk_size=2048, overlap=1042, drop_last=False),\n",
    "        \"semantic\": SemanticChunker(chunk_char_limit=120)\n",
    "    },\n",
    "    \"retrievers\": {\n",
    "        \"bm25\": BM25Retriever,\n",
    "        # \"dpr\": DPRRetriever,\n",
    "        # \"colbert\": ColBERTRetriever,\n",
    "        # \"hybrid\": HybridRetriever\n",
    "    },\n",
    "    \"top_k\":[5, 10, 20, 50]\n",
    "}\n",
    "\n",
    "strategies2 = {\n",
    "    \"chunkers\": {\n",
    "        \"fixed_128\": FixedChunker(chunk_size=128, drop_last=False),\n",
    "        \"overlapping_256\": OverlappingChunker(chunk_size=2048, overlap=256, drop_last=False),\n",
    "        \"semantic\": SemanticChunker(chunk_char_limit=120)\n",
    "    },\n",
    "    \"retrievers\": {\n",
    "        \"tf_idf\": TFIDFRetriever,\n",
    "        \"bm25\": BM25Retriever,\n",
    "        # \"dpr\": DPRRetriever,\n",
    "        \"colbert\": ColBERTRetriever,\n",
    "        \"hybrid\": HybridRetriever\n",
    "    },\n",
    "    \"top_k\":[5]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "results, overall_retriever_results = benchmark_all(strategies, documents, qa_pairs, top_k)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print results\n",
    "for res in overall_retriever_results:\n",
    "    print(\"Result\", res)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikunjagarwal/Desktop/598_project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/nikunjagarwal/Desktop/598_project/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retriever': 'bm25', 'chunker': 'fixed', 'initialize_latency': 0.08623003959655762, 'retrieve_latency': 0.0012904644012451173, 'chunking_latency': 0.01131892204284668, 'accuracy': 40.0}\n",
      "{'retriever': 'colbert', 'chunker': 'fixed', 'initialize_latency': 82.11048603057861, 'retrieve_latency': 0.11404080390930176, 'chunking_latency': 0.01131892204284668, 'accuracy': 20.0}\n",
      "{'retriever': 'hybrid', 'chunker': 'fixed', 'initialize_latency': 118.72547626495361, 'retrieve_latency': 0.04831962585449219, 'chunking_latency': 0.01131892204284668, 'accuracy': 20.0}\n",
      "{'retriever': 'bm25', 'chunker': 'overlapping', 'initialize_latency': 0.23556900024414062, 'retrieve_latency': 0.0032613277435302734, 'chunking_latency': 0.04497098922729492, 'accuracy': 40.0}\n",
      "{'retriever': 'colbert', 'chunker': 'overlapping', 'initialize_latency': 136.29622983932495, 'retrieve_latency': 0.046083259582519534, 'chunking_latency': 0.04497098922729492, 'accuracy': 20.0}\n",
      "{'retriever': 'hybrid', 'chunker': 'overlapping', 'initialize_latency': 123.12539911270142, 'retrieve_latency': 0.04539752006530762, 'chunking_latency': 0.04497098922729492, 'accuracy': 40.0}\n",
      "{'retriever': 'bm25', 'chunker': 'semantic', 'initialize_latency': 0.09370684623718262, 'retrieve_latency': 0.0011279582977294922, 'chunking_latency': 0.028107166290283203, 'accuracy': 40.0}\n",
      "{'retriever': 'colbert', 'chunker': 'semantic', 'initialize_latency': 90.05450510978699, 'retrieve_latency': 0.0906479835510254, 'chunking_latency': 0.028107166290283203, 'accuracy': 20.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 26\u001B[0m\n\u001B[1;32m      9\u001B[0m top_k \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m\n\u001B[1;32m     11\u001B[0m strategies \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchunkers\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m     13\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfixed\u001B[39m\u001B[38;5;124m\"\u001B[39m: FixedChunker(chunk_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m, drop_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     22\u001B[0m     }\n\u001B[1;32m     23\u001B[0m }\n\u001B[0;32m---> 26\u001B[0m results, overall_retriever_results \u001B[38;5;241m=\u001B[39m \u001B[43mbenchmark_all\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstrategies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mqa_pairs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# Print results\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m overall_retriever_results:\n",
      "Cell \u001B[0;32mIn[2], line 34\u001B[0m, in \u001B[0;36mbenchmark_all\u001B[0;34m(strategies, documents, qas, top_k)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m retriever_name, retriever_class \u001B[38;5;129;01min\u001B[39;00m strategies[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mretrievers\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m     33\u001B[0m     retriever_initialize_start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m---> 34\u001B[0m     retriever \u001B[38;5;241m=\u001B[39m \u001B[43mretriever_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m     retriever_initialize_end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     36\u001B[0m     retriever_initialize_latency \u001B[38;5;241m=\u001B[39m retriever_initialize_end_time \u001B[38;5;241m-\u001B[39m retriever_initialize_start_time\n",
      "File \u001B[0;32m~/Desktop/598_project/retrievers/hybrid_retriever.py:8\u001B[0m, in \u001B[0;36mHybridRetriever.__init__\u001B[0;34m(self, documents)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, documents):\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;66;03m# Initialize both BM25 and Dense retrievers\u001B[39;00m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbm25_retriever \u001B[38;5;241m=\u001B[39m BM25Retriever(documents)\n\u001B[0;32m----> 8\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolbert_retriever \u001B[38;5;241m=\u001B[39m \u001B[43mColBERTRetriever\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/598_project/retrievers/colbert_retriever.py:10\u001B[0m, in \u001B[0;36mColBERTRetriever.__init__\u001B[0;34m(self, documents)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m SentenceTransformer(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbert-base-nli-mean-tokens\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdocuments \u001B[38;5;241m=\u001B[39m documents\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdocument_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/598_project/retrievers/colbert_retriever.py:14\u001B[0m, in \u001B[0;36mColBERTRetriever.encode_documents\u001B[0;34m(self, documents)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mencode_documents\u001B[39m(\u001B[38;5;28mself\u001B[39m, documents):\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# Token-level embeddings via Sentence-BERT\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(embeddings)\n",
      "File \u001B[0;32m~/Desktop/598_project/.venv/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:720\u001B[0m, in \u001B[0;36mSentenceTransformer.encode\u001B[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    718\u001B[0m             \u001B[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001B[39;00m\n\u001B[1;32m    719\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m convert_to_numpy:\n\u001B[0;32m--> 720\u001B[0m                 embeddings \u001B[38;5;241m=\u001B[39m \u001B[43membeddings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    722\u001B[0m         all_embeddings\u001B[38;5;241m.\u001B[39mextend(embeddings)\n\u001B[1;32m    724\u001B[0m all_embeddings \u001B[38;5;241m=\u001B[39m [all_embeddings[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39margsort(length_sorted_idx)]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "a583a68580938af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T06:53:09.865562Z",
     "start_time": "2025-05-01T03:03:11.563165Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract values\n",
    "retrievers = [entry['retriever'] for entry in overall_retriever_results]\n",
    "init_latencies = [entry['initialize_latency'] for entry in overall_retriever_results]\n",
    "retrieve_latencies = [entry['retrieve_latency'] for entry in overall_retriever_results]\n",
    "chunk_latencies = [entry['chunking_latency'] for entry in overall_retriever_results]\n",
    "accuracies = [entry['accuracy'] for entry in overall_retriever_results]\n",
    "\n",
    "x = np.arange(len(retrievers))\n",
    "width = 0.2\n",
    "\n",
    "# Create subplot for latency\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Latency bars\n",
    "ax1.bar(x - width, init_latencies, width, label='Init Latency')\n",
    "ax1.bar(x, retrieve_latencies, width, label='Retrieve Latency')\n",
    "ax1.bar(x + width, chunk_latencies, width, label='Chunking Latency')\n",
    "\n",
    "ax1.set_xlabel('Retriever')\n",
    "ax1.set_ylabel('Latency (seconds)')\n",
    "ax1.set_title('Retriever Latency Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(retrievers)\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Create secondary y-axis for accuracy\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(x, accuracies, color='green', marker='o', linewidth=2, label='Accuracy (%)')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'Who was the man behind The Chipmunks?', 'answer': 'david seville'}]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd4fbc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed-length chunks (20 tokens each):\n",
      "  01. 'The sky appears blue because molecules in the air scatter blue light from the sun more than they scatter red'\n",
      "  02. 'light. This phenomenon is called Rayleigh scattering. When the sun is lower in the sky, the light has to pass'\n",
      "  03. 'through more atmosphere, so more blue and green light is scattered away, leaving the reds and oranges we see at'\n",
      "\n",
      "Overlapping chunks (24 tokens, 8-token overlap):\n",
      "  01. 'The sky appears blue because molecules in the air scatter blue light from the sun more than they scatter red light. This phenomenon is'\n",
      "  02. 'than they scatter red light. This phenomenon is called Rayleigh scattering. When the sun is lower in the sky, the light has to pass'\n",
      "  03. 'in the sky, the light has to pass through more atmosphere, so more blue and green light is scattered away, leaving the reds and'\n",
      "  04. 'light is scattered away, leaving the reds and oranges we see at sunrise and sunset.'\n",
      "\n",
      "Semantic chunks (~120 chars, sentence-aware):\n",
      "  01. 'The sky appears blue because molecules in the air scatter blue light from the sun more than they scatter red light.'\n",
      "  02. 'This phenomenon is called Rayleigh scattering.'\n",
      "  03. 'When the sun is lower in the sky, the light has to pass through more atmosphere, so more blue and green light is scattered away, leaving the reds and oranges we see at sunrise and sunset.'\n"
     ]
    }
   ],
   "source": [
    "# --- Example text ----------------------------------------------------------\n",
    "doc = (\n",
    "    \"The sky appears blue because molecules in the air scatter blue light from \"\n",
    "    \"the sun more than they scatter red light. This phenomenon is called Rayleigh \"\n",
    "    \"scattering. When the sun is lower in the sky, the light has to pass through \"\n",
    "    \"more atmosphere, so more blue and green light is scattered away, leaving the \"\n",
    "    \"reds and oranges we see at sunrise and sunset.\"\n",
    ")\n",
    "\n",
    "# --- Import the chunkers ---------------------------------------------------\n",
    "from chunkers import FixedChunker, OverlappingChunker, SemanticChunker\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Fixed-length chunking: split every 20 tokens (drop tail if < 20 tokens)\n",
    "fixed_chunker = FixedChunker(chunk_size=20, drop_last=True)\n",
    "fixed_chunks = fixed_chunker.chunk(doc)\n",
    "\n",
    "# 2. Overlapping windows: 24-token windows with 8-token overlap\n",
    "overlap_chunker = OverlappingChunker(chunk_size=24, overlap=8, drop_last=False)\n",
    "overlap_chunks = overlap_chunker.chunk(doc)\n",
    "\n",
    "# 3. Semantic packing: keep whole sentences, ~120 characters per chunk\n",
    "semantic_chunker = SemanticChunker(chunk_char_limit=120)\n",
    "semantic_chunks = semantic_chunker.chunk(doc)\n",
    "\n",
    "# --- Inspect the output ----------------------------------------------------\n",
    "print(\"Fixed-length chunks (20 tokens each):\")\n",
    "for i, c in enumerate(fixed_chunks, 1):\n",
    "    print(f\"  {i:02d}. {c!r}\")\n",
    "\n",
    "print(\"\\nOverlapping chunks (24 tokens, 8-token overlap):\")\n",
    "for i, c in enumerate(overlap_chunks, 1):\n",
    "    print(f\"  {i:02d}. {c!r}\")\n",
    "\n",
    "print(\"\\nSemantic chunks (~120 chars, sentence-aware):\")\n",
    "for i, c in enumerate(semantic_chunks, 1):\n",
    "    print(f\"  {i:02d}. {c!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39985b2e30f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 0.  Imports & one-time setup\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict\n",
    "import json, time\n",
    "\n",
    "# local modules\n",
    "from chunkers import FixedChunker, OverlappingChunker, SemanticChunker\n",
    "from retrievers import bm25_retriever, dense_retriever, hybrid_retriever\n",
    "from retrievers.dense_retriever import DPRRetriever  # example alias\n",
    "from utils.evaluation import exact_match, f1_score            # already in your repo\n",
    "from utils.timing import Timer                                 # already in your repo\n",
    "\n",
    "# external (install if missing)\n",
    "#   pip install rank-bm25 sentence-transformers faiss-cpu\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  Load a toy corpus\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "DOCS: Dict[str, str] = {\n",
    "    \"doc1\": Path(\"data/doc1.txt\").read_text(encoding=\"utf-8\"),\n",
    "    \"doc2\": Path(\"data/doc2.txt\").read_text(encoding=\"utf-8\"),\n",
    "    \"doc3\": Path(\"data/doc3.txt\").read_text(encoding=\"utf-8\"),\n",
    "}\n",
    "print(f\"Loaded {len(DOCS)} raw documents\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  Choose **one** chunker recipe for this run\n",
    "#     (Swap these objects in a loop if you’re doing a grid-search)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "chunker = FixedChunker(chunk_size=128, drop_last=False)\n",
    "# chunker = OverlappingChunker(chunk_size=256, overlap=64)\n",
    "# chunker = SemanticChunker(chunk_char_limit=1500)\n",
    "\n",
    "# 2-b.  Chunk every document → corpus_chunks[id] = [chunk0, …]\n",
    "corpus_chunks: Dict[str, List[str]] = {\n",
    "    doc_id: chunker.chunk(txt) for doc_id, txt in DOCS.items()\n",
    "}\n",
    "flat_chunks = [c for chs in corpus_chunks.values() for c in chs]\n",
    "print(\n",
    "    f\"Chunked into {len(flat_chunks):,} total passages \"\n",
    "    f\"(avg {len(flat_chunks)/len(DOCS):.1f} per doc)\"\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  Build the four retrieval back-ends on **exactly the same chunk set**\n",
    "#     The dense & hybrid examples assume a SentenceTransformer-based DPR.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3-a. BM25 (sparse)\n",
    "bm25_index = BM25Okapi([c.split() for c in flat_chunks])\n",
    "\n",
    "# 3-b. Dense Passage Retriever\n",
    "dpr_model = SentenceTransformer(\"facebook-dpr-ctx_encoder-multiset-base\")\n",
    "dpr_index = dense_retriever.build_faiss_index(flat_chunks, dpr_model)  # your util\n",
    "\n",
    "# 3-c. ColBERT (late interaction)  ← placeholder call\n",
    "colbert_retriever = dense_retriever.ColBERTIndexer().fit(flat_chunks)\n",
    "\n",
    "# 3-d. Hybrid (= BM25 + DPR scores)\n",
    "hybrid = hybrid_retriever.Hybrid(bm25_index, dpr_index, alpha=0.4)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4.  Query loop + simple QA evaluation\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "qa_pairs = json.loads(Path(\"data/dev_qas.json\").read_text())[:50]  # small dev slice\n",
    "K = 5\n",
    "\n",
    "results_by_system = defaultdict(list)\n",
    "\n",
    "for q in qa_pairs:\n",
    "    query, gold = q[\"question\"], q[\"answer\"]  # gold answer string\n",
    "\n",
    "    with Timer() as t:\n",
    "        top_chunks = bm25_index.get_top_n(query.split(), flat_chunks, n=K)\n",
    "    results_by_system[\"bm25\"].append(\n",
    "        {\"latency_ms\": t.ms, \"em\": exact_match(gold, top_chunks)}\n",
    "    )\n",
    "\n",
    "    with Timer() as t:\n",
    "        top_chunks = dense_retriever.search_faiss(query, dpr_index, dpr_model, top_k=K)\n",
    "    results_by_system[\"dpr\"].append(\n",
    "        {\"latency_ms\": t.ms, \"em\": exact_match(gold, top_chunks)}\n",
    "    )\n",
    "\n",
    "    with Timer() as t:\n",
    "        top_chunks = colbert_retriever.search(query, top_k=K)\n",
    "    results_by_system[\"colbert\"].append(\n",
    "        {\"latency_ms\": t.ms, \"em\": exact_match(gold, top_chunks)}\n",
    "    )\n",
    "\n",
    "    with Timer() as t:\n",
    "        top_chunks = hybrid.search(query, top_k=K)\n",
    "    results_by_system[\"hybrid\"].append(\n",
    "        {\"latency_ms\": t.ms, \"em\": exact_match(gold, top_chunks)}\n",
    "    )\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5.  Aggregate & display – how *this* chunker impacted each retriever\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def summarise(rows):\n",
    "    return {\n",
    "        \"EM\": sum(r[\"em\"] for r in rows) / len(rows),\n",
    "        \"Latency (ms)\": sum(r[\"latency_ms\"] for r in rows) / len(rows),\n",
    "    }\n",
    "\n",
    "summary = {name: summarise(rows) for name, rows in results_by_system.items()}\n",
    "print(\"\\n===  Chunker:\", chunker.__class__.__name__, \" ===\")\n",
    "for system, metrics in summary.items():\n",
    "    print(f\"{system:7s} | EM={metrics['EM']:.3f} | Latency≈{metrics['Latency (ms)']:.1f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4bb226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load] TriviaQA from data/triviaqa-unfiltered/unfiltered-web-dev.json\n",
      "[load] kept 250 non-empty docs, 100 QA pairs\n",
      "[chunk] FixedChunker: 250 chunks (≈1.0 per doc)\n",
      "[dpr] encoding chunks …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:01<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dpr] Faiss index: 250 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "retrieving: 100%|██████████| 100/100 [00:04<00:00, 22.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESULTS (Exact-Match evidence recall) ===\n",
      "BM25 | EM@5: 0.010 | avg latency 0.8 ms\n",
      "DPR  | EM@5: 0.000 | avg latency 43.2 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Minimal Retrieval-only demo on TriviaQA-unfiltered.\n",
    "\n",
    "Dependencies:\n",
    "  pip install rank-bm25 sentence-transformers faiss-cpu tqdm\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, gzip, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Retrieval libs\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss                           # CPU version is OK\n",
    "\n",
    "# Your chunkers (put the three modules in rag_pipeline/chunkers/)\n",
    "from chunkers import FixedChunker, OverlappingChunker, SemanticChunker\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 1. TriviaQA loader (improved)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def load_triviaqa(\n",
    "    dataset_path: str,\n",
    "    max_docs: int = 300,\n",
    "    max_qa_pairs: int = 100,\n",
    ") -> Tuple[Dict[str, str], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        docs   : {doc_id: raw_text}\n",
    "        qa_pairs: [{\"question\": str, \"answer\": str}, …]\n",
    "    \"\"\"\n",
    "    print(f\"[load] TriviaQA from {dataset_path}\")\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)[\"Data\"]\n",
    "\n",
    "    root = Path(dataset_path).parent\n",
    "    docs: Dict[str, str] = {}\n",
    "    qa_pairs: List[Dict] = []\n",
    "\n",
    "    def read_evidence(file_path: Path) -> str:\n",
    "        if not file_path.exists():\n",
    "            return \"\"\n",
    "        # Many evidence files are .gz\n",
    "        try:\n",
    "            if file_path.suffix == \".gz\":\n",
    "                with gzip.open(file_path, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as g:\n",
    "                    return g.read()\n",
    "            return file_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def add_doc(doc_info, prefix: str, j: int):\n",
    "        if len(docs) >= max_docs:\n",
    "            return\n",
    "        doc_id = f\"{prefix}_{j}\"\n",
    "        txt = \"\"\n",
    "        if doc_info.get(\"Filename\"):\n",
    "            txt = read_evidence(root / doc_info[\"Filename\"])\n",
    "        # Fallbacks\n",
    "        txt = txt or doc_info.get(\"Snippet\", \"\") or doc_info.get(\"Title\", \"\") or doc_info.get(\"Url\", \"\")\n",
    "        docs[doc_id] = txt\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        if i >= max_qa_pairs:\n",
    "            break\n",
    "\n",
    "        # QA\n",
    "        question = item[\"Question\"]\n",
    "        aliases = item[\"Answer\"].get(\"NormalizedAliases\") or []\n",
    "        gold = aliases[0] if aliases else item[\"Answer\"][\"NormalizedValue\"]\n",
    "        qa_pairs.append({\"question\": question, \"answer\": gold})\n",
    "\n",
    "        # Wiki evidence\n",
    "        for j, d in enumerate(item.get(\"EntityPages\", [])):\n",
    "            add_doc(d, f\"wiki_{d.get('Title','wiki')}\", j)\n",
    "        # Web search evidence\n",
    "        for j, d in enumerate(item.get(\"SearchResults\", [])):\n",
    "            add_doc(d, \"web\", j)\n",
    "\n",
    "    # Drop empties\n",
    "    docs = {k: v for k, v in docs.items() if v.strip()}\n",
    "    print(f\"[load] kept {len(docs)} non-empty docs, {len(qa_pairs)} QA pairs\")\n",
    "    return docs, qa_pairs\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Exact-Match helper\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def exact_match(gold: str, passages: List[str]) -> int:\n",
    "    g = gold.lower()\n",
    "    return int(any(g in p.lower() for p in passages))\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Main\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    # -------- paths & params ------------------------------------------------\n",
    "    TQA_JSON = \"data/triviaqa-unfiltered/unfiltered-web-dev.json\"   # edit if needed\n",
    "    MAX_DOCS = 300\n",
    "    MAX_QA   = 100\n",
    "    TOP_K    = 5\n",
    "\n",
    "    # -------- load ----------------------------------------------------------\n",
    "    docs, qa_pairs = load_triviaqa(TQA_JSON, MAX_DOCS, MAX_QA)\n",
    "    if not docs:\n",
    "        raise RuntimeError(\"No non-empty docs – check dataset path / permissions\")\n",
    "\n",
    "    # -------- choose chunker ------------------------------------------------\n",
    "    chunker = FixedChunker(chunk_size=128, drop_last=False)\n",
    "    # chunker = OverlappingChunker(chunk_size=256, overlap=64)\n",
    "    # chunker = SemanticChunker(chunk_char_limit=1500)\n",
    "\n",
    "    chunk_texts, chunk_to_doc = [], []\n",
    "    for doc_id, txt in docs.items():\n",
    "        for ch in chunker.chunk(txt):\n",
    "            chunk_texts.append(ch)\n",
    "            chunk_to_doc.append(doc_id)\n",
    "\n",
    "    print(f\"[chunk] {chunker.__class__.__name__}: {len(chunk_texts)} chunks \"\n",
    "          f\"(≈{len(chunk_texts)/len(docs):.1f} per doc)\")\n",
    "\n",
    "    # -------- BM25 ----------------------------------------------------------\n",
    "    bm25 = BM25Okapi([c.split() for c in chunk_texts])\n",
    "\n",
    "    # -------- DPR -----------------------------------------------------------\n",
    "    model = SentenceTransformer(\"facebook-dpr-ctx_encoder-multiset-base\")\n",
    "    print(\"[dpr] encoding chunks …\")\n",
    "    ctx_emb = model.encode(\n",
    "        chunk_texts,\n",
    "        batch_size=64,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    index = faiss.IndexFlatIP(ctx_emb.shape[1])\n",
    "    index.add(ctx_emb)\n",
    "    print(f\"[dpr] Faiss index: {index.ntotal} vectors\")\n",
    "\n",
    "    # -------- retrieval loop ------------------------------------------------\n",
    "    bm25_hits = dpr_hits = 0\n",
    "    bm25_lat, dpr_lat = [], []\n",
    "\n",
    "    for qa in tqdm(qa_pairs, desc=\"retrieving\"):\n",
    "        q, gold = qa[\"question\"], qa[\"answer\"]\n",
    "\n",
    "        # BM25\n",
    "        t0 = time.perf_counter()\n",
    "        ids = bm25.get_top_n(q.split(), list(range(len(chunk_texts))), n=TOP_K)\n",
    "        bm25_lat.append((time.perf_counter() - t0) * 1e3)\n",
    "        bm25_hits += exact_match(gold, [chunk_texts[i] for i in ids])\n",
    "\n",
    "        # DPR\n",
    "        t0 = time.perf_counter()\n",
    "        q_emb = model.encode([q], normalize_embeddings=True)\n",
    "        _, idxs = index.search(q_emb, TOP_K)\n",
    "        dpr_lat.append((time.perf_counter() - t0) * 1e3)\n",
    "        dpr_hits += exact_match(gold, [chunk_texts[i] for i in idxs[0]])\n",
    "\n",
    "    # -------- summary -------------------------------------------------------\n",
    "    n = len(qa_pairs)\n",
    "    print(\"\\n=== RESULTS (Exact-Match evidence recall) ===\")\n",
    "    print(f\"BM25 | EM@{TOP_K}: {bm25_hits/n:.3f} | avg latency {np.mean(bm25_lat):.1f} ms\")\n",
    "    print(f\"DPR  | EM@{TOP_K}: {dpr_hits/n:.3f} | avg latency {np.mean(dpr_lat):.1f} ms\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
